import * as path from 'path';
import * as fs from 'fs';
import { nodewhisper } from 'nodejs-whisper';

export interface TranscriptSegment {
  startTime: number;
  endTime: number;
  text: string;
  confidence: number;
}

export interface TranscriptionOptions {
  modelName?: string;
  language?: string;
  wordTimestamps?: boolean;
  outputFormats?: {
    srt?: boolean;
    json?: boolean;
    text?: boolean;
  };
}

export interface TranscriptionResult {
  segments: TranscriptSegment[];
  language?: string;
  duration: number;
}

export class TranscriptionService {
  private defaultOptions: TranscriptionOptions = {
    modelName: 'base.en',
    language: 'en',
    wordTimestamps: true,
    outputFormats: {
      srt: true,
      json: true,
      text: false,
    },
  };

  /**
   * Transcribe audio file using Whisper
   * @param audioPath Path to the audio file (WAV format, 16kHz)
   * @param options Transcription options
   * @returns Promise with transcription results
   */
  async transcribeAudio(
    audioPath: string,
    options: TranscriptionOptions = {}
  ): Promise<TranscriptionResult> {
    try {
      console.log(`üéôÔ∏è TranscriptionService: Starting transcription of ${audioPath}`);
      
      const mergedOptions = { ...this.defaultOptions, ...options };
      console.log(`üéôÔ∏è TranscriptionService: Using model: ${mergedOptions.modelName}`);

      // Verify audio file exists
      if (!fs.existsSync(audioPath)) {
        throw new Error(`Audio file not found: ${audioPath}`);
      }

      // Get audio file duration for validation
      const stats = fs.statSync(audioPath);
      console.log(`üéôÔ∏è TranscriptionService: Audio file size: ${(stats.size / 1024 / 1024).toFixed(2)} MB`);

      // Configure nodejs-whisper options
      const whisperOptions = {
        modelName: mergedOptions.modelName!,
        autoDownloadModelName: mergedOptions.modelName!,
        removeWavFileAfterTranscription: false, // We'll handle cleanup separately
        withCuda: false, // Use CPU for compatibility
        logger: console,
        whisperOptions: {
          outputInCsv: false,
          outputInJson: mergedOptions.outputFormats?.json || true,
          outputInJsonFull: false,
          outputInLrc: false,
          outputInSrt: mergedOptions.outputFormats?.srt || true,
          outputInText: mergedOptions.outputFormats?.text || false,
          outputInVtt: false,
          outputInWords: false,
          translateToEnglish: mergedOptions.language !== 'en',
          wordTimestamps: mergedOptions.wordTimestamps || true,
          timestamps_length: 20, // Amount of dialogue per timestamp pair
          splitOnWord: true, // Split on word rather than token
        },
      };

      console.log(`üéôÔ∏è TranscriptionService: Calling nodejs-whisper...`);
      
      // Call nodejs-whisper
      await nodewhisper(audioPath, whisperOptions);

      console.log(`‚úÖ TranscriptionService: Whisper transcription completed`);

      // Parse the generated files to extract segments
      const segments = await this.parseTranscriptionOutput(audioPath);
      
      // Calculate duration (we can get this from the audio file or segments)
      const duration = segments.length > 0 ? 
        Math.max(...segments.map(s => s.endTime)) : 0;

      const result: TranscriptionResult = {
        segments,
        language: mergedOptions.language,
        duration,
      };

      console.log(`üéôÔ∏è TranscriptionService: Found ${segments.length} transcript segments`);
      return result;

    } catch (error) {
      console.error('‚ùå TranscriptionService: Error during transcription:', error);
      throw new Error(`Transcription failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }

  /**
   * Parse the output files generated by nodejs-whisper to extract transcript segments
   * @param audioPath Original audio file path (used to find output files)
   * @returns Array of transcript segments
   */
  private async parseTranscriptionOutput(audioPath: string): Promise<TranscriptSegment[]> {
    try {
      // nodejs-whisper generates files with the same base name as input
      const baseName = path.basename(audioPath, path.extname(audioPath));
      const audioDir = path.dirname(audioPath);
      
      // Try to find JSON output file
      const jsonPath = path.join(audioDir, `${baseName}.json`);
      const srtPath = path.join(audioDir, `${baseName}.srt`);

      let segments: TranscriptSegment[] = [];

      // Try parsing JSON first (more detailed)
      if (fs.existsSync(jsonPath)) {
        console.log(`üéôÔ∏è TranscriptionService: Parsing JSON output: ${jsonPath}`);
        segments = await this.parseJsonOutput(jsonPath);
        
        // Clean up JSON file
        try {
          fs.unlinkSync(jsonPath);
        } catch (e) {
          console.warn(`‚ö†Ô∏è TranscriptionService: Could not delete JSON file: ${e}`);
        }
      }
      // Fallback to SRT parsing
      else if (fs.existsSync(srtPath)) {
        console.log(`üéôÔ∏è TranscriptionService: Parsing SRT output: ${srtPath}`);
        segments = await this.parseSrtOutput(srtPath);
        
        // Clean up SRT file
        try {
          fs.unlinkSync(srtPath);
        } catch (e) {
          console.warn(`‚ö†Ô∏è TranscriptionService: Could not delete SRT file: ${e}`);
        }
      }
      else {
        console.warn(`‚ö†Ô∏è TranscriptionService: No output files found for ${audioPath}`);
        // Return empty segments - transcription may have failed silently
        return [];
      }

      return segments;

    } catch (error) {
      console.error('‚ùå TranscriptionService: Error parsing transcription output:', error);
      return [];
    }
  }

  /**
   * Parse JSON output from nodejs-whisper
   * @param jsonPath Path to JSON output file
   * @returns Array of transcript segments
   */
  private async parseJsonOutput(jsonPath: string): Promise<TranscriptSegment[]> {
    try {
      const jsonContent = fs.readFileSync(jsonPath, 'utf8');
      const data = JSON.parse(jsonContent);
      
      const segments: TranscriptSegment[] = [];
      
      // Parse transcription data structure
      if (data.transcription && Array.isArray(data.transcription)) {
        for (const item of data.transcription) {
          if (item.timestamps && item.timestamps.from !== undefined && item.timestamps.to !== undefined) {
            segments.push({
              startTime: item.timestamps.from / 1000, // Convert from ms to seconds
              endTime: item.timestamps.to / 1000,
              text: item.text.trim(),
              confidence: item.confidence || 0.9, // Default confidence if not provided
            });
          }
        }
      }

      console.log(`üéôÔ∏è TranscriptionService: Parsed ${segments.length} segments from JSON`);
      return segments;

    } catch (error) {
      console.error('‚ùå TranscriptionService: Error parsing JSON output:', error);
      return [];
    }
  }

  /**
   * Parse SRT output from nodejs-whisper as fallback
   * @param srtPath Path to SRT output file
   * @returns Array of transcript segments
   */
  private async parseSrtOutput(srtPath: string): Promise<TranscriptSegment[]> {
    try {
      const srtContent = fs.readFileSync(srtPath, 'utf8');
      const segments: TranscriptSegment[] = [];
      
      // Parse SRT format
      const blocks = srtContent.split('\n\n').filter(block => block.trim());
      
      for (const block of blocks) {
        const lines = block.split('\n');
        if (lines.length >= 3) {
          const timeRange = lines[1];
          const text = lines.slice(2).join(' ').trim();
          
          // Parse timestamp format: 00:00:00,000 --> 00:00:05,000
          const timeMatch = timeRange.match(/(\d{2}):(\d{2}):(\d{2}),(\d{3})\s*-->\s*(\d{2}):(\d{2}):(\d{2}),(\d{3})/);
          
          if (timeMatch) {
            const startTime = 
              parseInt(timeMatch[1]) * 3600 + // hours
              parseInt(timeMatch[2]) * 60 +   // minutes
              parseInt(timeMatch[3]) +        // seconds
              parseInt(timeMatch[4]) / 1000;  // milliseconds
              
            const endTime = 
              parseInt(timeMatch[5]) * 3600 + // hours
              parseInt(timeMatch[6]) * 60 +   // minutes
              parseInt(timeMatch[7]) +        // seconds
              parseInt(timeMatch[8]) / 1000;  // milliseconds
            
            segments.push({
              startTime,
              endTime,
              text,
              confidence: 0.9, // Default confidence for SRT parsing
            });
          }
        }
      }

      console.log(`üéôÔ∏è TranscriptionService: Parsed ${segments.length} segments from SRT`);
      return segments;

    } catch (error) {
      console.error('‚ùå TranscriptionService: Error parsing SRT output:', error);
      return [];
    }
  }

  /**
   * Get available Whisper models
   * @returns List of available model names
   */
  getAvailableModels(): string[] {
    return [
      'tiny',
      'tiny.en',
      'base',
      'base.en',
      'small',
      'small.en',
      'medium',
      'medium.en',
      'large-v1',
      'large',
      'large-v3-turbo',
    ];
  }
} 